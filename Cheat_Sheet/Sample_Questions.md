# INFO 7375: Comprehensive LLM Assessment
## 50 Questions on Large Language Models (2024-2025 Edition)

---

## Section 1: LLM Fundamentals & Architecture

### Question 1: Transformer Architecture
Which components are fundamental to the Transformer architecture used in modern LLMs?

**Options:**
1. **True/False**: Self-attention mechanism
2. **True/False**: Convolutional layers
3. **True/False**: Positional encodings
4. **True/False**: Feed-forward neural networks
5. **True/False**: Recurrent connections

**Explanation:**
1. **True**: Self-attention is the core mechanism that allows Transformers to weigh the importance of different tokens.
2. **False**: Transformers replaced CNNs with attention mechanisms; CNNs are used in vision models.
3. **True**: Positional encodings provide information about token position since attention has no inherent order.
4. **True**: Feed-forward networks process the output of attention layers.
5. **False**: Transformers explicitly avoid recurrent connections, processing sequences in parallel instead.

---

### Question 2: Tokenization
What is true about tokenization in LLMs?

**Options:**
1. **True/False**: Tokens are always complete words
2. **True/False**: Subword tokenization (BPE, WordPiece) is commonly used
3. **True/False**: Different models use different tokenizers
4. **True/False**: Token count affects API costs and context limits
5. **True/False**: One token always equals one character

**Explanation:**
1. **False**: Tokens can be subwords, parts of words, or even individual characters.
2. **True**: Byte-Pair Encoding (BPE) and WordPiece are standard tokenization methods.
3. **True**: GPT models use different tokenizers than Claude or Llama models.
4. **True**: API pricing is per token, and context windows are measured in tokens.
5. **False**: A token can be multiple characters; on average, 1 token â‰ˆ 0.75 words in English.

---

### Question 3: Context Windows
What accurately describes context windows in modern LLMs (2024-2025)?

**Options:**
1. **True/False**: Context windows are limited to 4K tokens in all models
2. **True/False**: Claude 4 supports up to 1 million tokens (beta)
3. **True/False**: Gemini 1.5 Pro offers 1M+ token context
4. **True/False**: Larger context windows always mean better performance
5. **True/False**: Context window determines how much text the model can process at once

**Explanation:**
1. **False**: Modern models support 32K to 1M+ tokens; 4K was an early GPT-3 limitation.
2. **True**: Claude 4 Sonnet offers 200K standard with 1M token beta availability.
3. **True**: Gemini 1.5 Pro supports up to 2 million tokens.
4. **False**: Larger contexts can suffer from "lost in the middle" problems and increased latency.
5. **True**: The context window is the total input and output the model can handle in one interaction.

---

### Question 4: Pre-training vs Fine-tuning
What distinguishes pre-training from fine-tuning in LLM development?

**Options:**
1. **True/False**: Pre-training uses massive, diverse datasets
2. **True/False**: Fine-tuning adapts the model to specific tasks or domains
3. **True/False**: Pre-training is computationally cheaper than fine-tuning
4. **True/False**: RLHF (Reinforcement Learning from Human Feedback) is a form of fine-tuning
5. **True/False**: Fine-tuned models cannot be further adapted

**Explanation:**
1. **True**: Pre-training uses trillions of tokens from diverse sources (web, books, code).
2. **True**: Fine-tuning specializes pre-trained models for specific use cases.
3. **False**: Pre-training requires vastly more compute (thousands of GPUs for months).
4. **True**: RLHF fine-tunes models based on human preferences for safety and helpfulness.
5. **False**: Models can be fine-tuned multiple times for different purposes.

---

### Question 5: Emergent Capabilities
What are considered emergent capabilities in LLMs?

**Options:**
1. **True/False**: Abilities that appear suddenly at certain model scales
2. **True/False**: Chain-of-thought reasoning
3. **True/False**: Few-shot learning
4. **True/False**: These capabilities are explicitly programmed
5. **True/False**: In-context learning without gradient updates

**Explanation:**
1. **True**: Some abilities emerge unpredictably as model size increases.
2. **True**: Multi-step reasoning emerged in larger models without specific training.
3. **True**: Learning from examples in the prompt emerged at scale.
4. **False**: These capabilities weren't explicitly programmed; they emerged from scale and training.
5. **True**: Models can learn new tasks from prompt examples without parameter updates.

---

## Section 2: Multimodal Capabilities

### Question 6: Multimodal LLMs
What defines multimodal LLMs in 2024-2025?

**Options:**
1. **True/False**: They can only process text
2. **True/False**: They integrate vision encoders for image understanding
3. **True/False**: Examples include GPT-4V, Claude 3+, and Gemini
4. **True/False**: They use modality interfaces to connect different encoders
5. **True/False**: Audio and video processing are now standard capabilities

**Explanation:**
1. **False**: Multimodal LLMs process multiple modalities (text, images, audio, video).
2. **True**: Vision encoders (like CLIP) are integrated with language models.
3. **True**: These are leading multimodal models in production.
4. **True**: Interfaces bridge different modalities into a unified representation for the LLM.
5. **True**: Advanced models like Gemini 2.5 and GPT-4o support audio and video natively.

---

### Question 7: Vision-Language Understanding
What can modern multimodal LLMs do with images?

**Options:**
1. **True/False**: Describe image content in detail
2. **True/False**: Answer questions about images
3. **True/False**: Perform optical character recognition (OCR)
4. **True/False**: Generate images from text descriptions
5. **True/False**: Count objects and understand spatial relationships

**Explanation:**
1. **True**: Models can generate detailed image descriptions.
2. **True**: Visual question answering is a core capability.
3. **True**: OCR-free text extraction from images is now standard.
4. **False**: This requires separate image generation models (Stable Diffusion, DALL-E, Midjourney).
5. **True**: Advanced models can count objects and understand spatial relationships.

---

### Question 8: Video Understanding
What capabilities do LLMs have for video content in 2024-2025?

**Options:**
1. **True/False**: Frame-by-frame analysis with temporal understanding
2. **True/False**: Video summarization
3. **True/False**: Action recognition and event detection
4. **True/False**: All LLMs can process video equally well
5. **True/False**: Long video understanding (hours of content) is possible

**Explanation:**
1. **True**: Models can analyze video frames while maintaining temporal context.
2. **True**: Summarizing video content is a standard capability.
3. **True**: Identifying actions and events across video timelines is supported.
4. **False**: Specialized models like Gemini 2.5 and Qwen-VL excel at video; not all models support it.
5. **True**: With extended contexts (1M+ tokens), models can process hours of video content.

---

## Section 3: Prompt Engineering Fundamentals

### Question 9: Temperature Parameter
How does the temperature parameter affect LLM outputs?

**Options:**
1. **True/False**: Temperature = 0 produces deterministic outputs
2. **True/False**: Higher temperature (e.g., 1.0-2.0) increases creativity and randomness
3. **True/False**: Temperature controls the probability distribution over next tokens
4. **True/False**: Lower temperature is better for factual, precise tasks
5. **True/False**: Temperature affects the model's knowledge base

**Explanation:**
1. **True**: Temperature = 0 selects the highest probability token deterministically.
2. **True**: Higher temperature flattens the probability distribution, allowing more diverse outputs.
3. **True**: It scales the logits before applying softmax to control randomness.
4. **True**: Low temperature (0-0.3) is ideal for tasks requiring consistency and accuracy.
5. **False**: Temperature only affects sampling; it doesn't change the model's training or knowledge.

---

### Question 10: Top-p (Nucleus) Sampling
What is top-p sampling in LLM generation?

**Options:**
1. **True/False**: It samples from the top p% of most likely tokens
2. **True/False**: It creates a dynamic vocabulary based on cumulative probability
3. **True/False**: p=1.0 considers all possible tokens
4. **True/False**: Lower p values (e.g., 0.1) produce more focused outputs
5. **True/False**: Top-p and temperature are mutually exclusive

**Explanation:**
1. **True**: Top-p selects tokens whose cumulative probability reaches threshold p.
2. **True**: The vocabulary size adapts based on the probability distribution.
3. **True**: p=1.0 includes all tokens, equivalent to no filtering.
4. **True**: Lower p values restrict sampling to high-probability tokens, reducing randomness.
5. **False**: Top-p and temperature can be used together to control output diversity.

---

### Question 11: Prompt Components
What are the essential components of an effective prompt?

**Options:**
1. **True/False**: Clear instruction or task description
2. **True/False**: Context and background information
3. **True/False**: Examples (few-shot learning)
4. **True/False**: Output format specification
5. **True/False**: Constraints and requirements

**Explanation:**
1. **True**: Clear instructions help the model understand what's expected.
2. **True**: Context provides necessary background for informed responses.
3. **True**: Examples demonstrate desired output format and style.
4. **True**: Specifying format (JSON, markdown, etc.) ensures usable outputs.
5. **True**: Constraints (length, tone, style) guide the model's generation.

---

### Question 12: Zero-shot vs Few-shot Learning
What distinguishes zero-shot from few-shot prompting?

**Options:**
1. **True/False**: Zero-shot provides no examples in the prompt
2. **True/False**: Few-shot includes 1-10 examples to guide the model
3. **True/False**: Few-shot generally produces better results for specific tasks
4. **True/False**: Zero-shot requires fine-tuning the model
5. **True/False**: Many-shot learning uses dozens or hundreds of examples

**Explanation:**
1. **True**: Zero-shot relies solely on instructions without examples.
2. **True**: Few-shot provides examples to demonstrate the desired pattern.
3. **True**: Examples significantly improve performance on specialized or nuanced tasks.
4. **False**: Zero-shot works with pre-trained models without any fine-tuning.
5. **True**: With extended contexts (100K+ tokens), many-shot learning is now possible.

---

## Section 4: Advanced Prompting Techniques

### Question 13: Chain-of-Thought (CoT) Prompting
What characterizes chain-of-thought prompting?

**Options:**
1. **True/False**: Encourages step-by-step reasoning
2. **True/False**: Improves performance on complex reasoning tasks
3. **True/False**: Triggered by phrases like "Let's think step by step"
4. **True/False**: Only works with models larger than 100B parameters
5. **True/False**: Shows intermediate reasoning steps before the final answer

**Explanation:**
1. **True**: CoT prompts the model to break down problems into steps.
2. **True**: Significantly improves mathematical, logical, and multi-step reasoning.
3. **True**: This phrase is a common CoT trigger in zero-shot scenarios.
4. **False**: CoT works with smaller models (7B+), though larger models show stronger effects.
5. **True**: Making reasoning explicit improves accuracy and interpretability.

---

### Question 14: Self-Consistency
What is self-consistency in LLM prompting?

**Options:**
1. **True/False**: Generating multiple reasoning paths for the same problem
2. **True/False**: Selecting the most common answer among multiple outputs
3. **True/False**: Requires only a single model call
4. **True/False**: Improves reliability on reasoning tasks
5. **True/False**: Combines with chain-of-thought prompting

**Explanation:**
1. **True**: The model generates several independent solutions with CoT.
2. **True**: The final answer is determined by majority vote across outputs.
3. **False**: Requires multiple model calls (typically 5-40) with different samples.
4. **True**: Averaging across reasoning paths reduces errors and improves accuracy.
5. **True**: Self-consistency typically uses CoT reasoning for each generation.

---

### Question 15: ReAct (Reasoning + Acting)
What is the ReAct prompting pattern?

**Options:**
1. **True/False**: Combines reasoning with action execution
2. **True/False**: Allows models to interact with external tools/APIs
3. **True/False**: Alternates between thinking and acting steps
4. **True/False**: Requires no external systems or functions
5. **True/False**: Used in agent-based LLM applications

**Explanation:**
1. **True**: ReAct interleaves reasoning traces with actions.
2. **True**: Models can call functions, search databases, or access APIs.
3. **True**: The pattern follows: Thought â†’ Action â†’ Observation â†’ Thought...
4. **False**: ReAct requires external systems for the model to interact with.
5. **True**: Agent frameworks (AutoGPT, LangChain Agents) use ReAct patterns.

---

### Question 16: Prompt Chaining
What is prompt chaining in LLM applications?

**Options:**
1. **True/False**: Breaking complex tasks into sequential prompts
2. **True/False**: Output from one prompt becomes input to the next
3. **True/False**: Improves accuracy on multi-step workflows
4. **True/False**: Requires all steps in a single prompt
5. **True/False**: Enables modular, debuggable AI systems

**Explanation:**
1. **True**: Complex tasks are decomposed into manageable sub-prompts.
2. **True**: Each step builds on previous outputs in a pipeline.
3. **True**: Focused prompts at each stage improve overall quality.
4. **False**: Chaining explicitly uses multiple separate prompts.
5. **True**: Each step can be tested, refined, and debugged independently.

---

### Question 17: Role-Based Prompting
How does role-based prompting improve LLM outputs?

**Options:**
1. **True/False**: Assigns the model a specific persona or expertise
2. **True/False**: Example: "You are an expert software engineer"
3. **True/False**: Influences the model's response style and depth
4. **True/False**: Has no effect on output quality
5. **True/False**: Can specify communication style and audience level

**Explanation:**
1. **True**: Role-based prompting frames the model's perspective and expertise.
2. **True**: This is a classic role-based prompt establishing expertise.
3. **True**: Roles influence vocabulary, depth, and approach to problems.
4. **False**: Role framing significantly impacts response quality and relevance.
5. **True**: Roles can specify "explain to a 5-year-old" or "write for technical experts."

---

## Section 5: Retrieval-Augmented Generation (RAG)

### Question 18: RAG Architecture
What are the core components of a RAG system?

**Options:**
1. **True/False**: Vector database for storing embeddings
2. **True/False**: Embedding model to vectorize text
3. **True/False**: Retrieval mechanism to find relevant documents
4. **True/False**: LLM to generate responses using retrieved context
5. **True/False**: RAG systems require fine-tuning the LLM

**Explanation:**
1. **True**: Vector databases (Pinecone, Weaviate, Chroma) store document embeddings.
2. **True**: Embedding models (sentence-transformers, OpenAI embeddings) create vectors.
3. **True**: Semantic search retrieves the most relevant documents for a query.
4. **True**: The LLM generates responses grounded in the retrieved documents.
5. **False**: RAG enhances pre-trained models without requiring fine-tuning.

---

### Question 19: Vector Embeddings
What are vector embeddings in the context of RAG?

**Options:**
1. **True/False**: Numerical representations of text in high-dimensional space
2. **True/False**: Similar texts have similar embeddings
3. **True/False**: Typically 384-1536 dimensions
4. **True/False**: Enable semantic search based on meaning, not just keywords
5. **True/False**: Embeddings must be regenerated for every query

**Explanation:**
1. **True**: Text is converted into dense vectors capturing semantic meaning.
2. **True**: Semantic similarity is measured by vector distance (cosine similarity).
3. **True**: Common dimensions are 384 (sentence-transformers), 768 (BERT), 1536 (OpenAI).
4. **True**: Vector search finds conceptually related content, not just keyword matches.
5. **False**: Only documents are embedded once; query embeddings are generated per search.

---

### Question 20: RAG Benefits
Why is RAG valuable for LLM applications?

**Options:**
1. **True/False**: Provides up-to-date information beyond training data
2. **True/False**: Reduces hallucination by grounding responses in sources
3. **True/False**: Enables access to private/proprietary data
4. **True/False**: Eliminates the need for context windows
5. **True/False**: More cost-effective than fine-tuning for knowledge updates

**Explanation:**
1. **True**: RAG retrieves current information from external sources.
2. **True**: Grounding in retrieved documents improves factual accuracy.
3. **True**: Organizations can query internal documents without training data exposure.
4. **False**: Retrieved documents still consume context window space.
5. **True**: Updating a vector database is cheaper than retraining/fine-tuning models.

---

### Question 21: Chunking Strategies
What are important considerations for document chunking in RAG?

**Options:**
1. **True/False**: Chunk size affects retrieval relevance
2. **True/False**: Overlapping chunks improve context preservation
3. **True/False**: All documents should use identical chunk sizes
4. **True/False**: Semantic chunking (by topic/section) can outperform fixed-size chunks
5. **True/False**: Chunk size must be smaller than the embedding model's limit

**Explanation:**
1. **True**: Too large loses precision; too small loses context.
2. **True**: Overlap (e.g., 10-20%) prevents information loss at boundaries.
3. **False**: Optimal chunk size varies by content type and use case.
4. **True**: Respecting document structure can improve retrieval quality.
5. **True**: Most embedding models have maximum token limits (512-8192 tokens).

---

## Section 6: Fine-Tuning & Adaptation

### Question 22: LoRA (Low-Rank Adaptation)
What is LoRA in the context of LLM fine-tuning?

**Options:**
1. **True/False**: A parameter-efficient fine-tuning method
2. **True/False**: Trains only a small set of additional parameters
3. **True/False**: Preserves the original model weights
4. **True/False**: Requires retraining all model parameters
5. **True/False**: Significantly reduces memory and compute requirements

**Explanation:**
1. **True**: LoRA adds trainable rank decomposition matrices to model layers.
2. **True**: Typically trains <1% of the total parameters.
3. **True**: Original weights remain frozen; only LoRA weights are updated.
4. **False**: LoRA specifically avoids retraining the full model.
5. **True**: Can fine-tune large models on consumer GPUs (e.g., RTX 4090).

---

### Question 23: RLHF (Reinforcement Learning from Human Feedback)
What role does RLHF play in LLM development?

**Options:**
1. **True/False**: Aligns model outputs with human preferences
2. **True/False**: Used to improve helpfulness, harmlessness, and honesty
3. **True/False**: Requires a reward model trained on human comparisons
4. **True/False**: Applied during pre-training on raw text
5. **True/False**: Used by ChatGPT, Claude, and most instruction-following models

**Explanation:**
1. **True**: RLHF fine-tunes models to match human values and preferences.
2. **True**: These are the "3 Hs" that RLHF optimizes for.
3. **True**: Humans rank outputs, training a reward model to guide RL optimization.
4. **False**: RLHF is a post-training alignment technique, not pre-training.
5. **True**: Most modern conversational LLMs use RLHF or similar alignment methods.

---

### Question 24: Instruction Tuning
What is instruction tuning in LLM training?

**Options:**
1. **True/False**: Fine-tuning on datasets of instructions and desired outputs
2. **True/False**: Teaches models to follow user instructions
3. **True/False**: Required before RLHF
4. **True/False**: Uses datasets like FLAN, Alpaca, or Dolly
5. **True/False**: Transforms base models into instruction-following assistants

**Explanation:**
1. **True**: Models learn from instruction-response pairs.
2. **True**: Instruction tuning is crucial for creating helpful assistants.
3. **True**: Typically applied before RLHF in the training pipeline.
4. **True**: These are prominent instruction-tuning datasets.
5. **True**: Base models become capable of following diverse user requests.

---

## Section 7: Production & Deployment

### Question 25: API vs Self-Hosted Models
What are key considerations when choosing between API and self-hosted LLMs?

**Options:**
1. **True/False**: APIs (OpenAI, Anthropic) offer easier scalability
2. **True/False**: Self-hosted models provide more control over data privacy
3. **True/False**: APIs eliminate infrastructure management
4. **True/False**: Self-hosting is always more cost-effective
5. **True/False**: APIs typically offer the most advanced models first

**Explanation:**
1. **True**: API providers handle scaling automatically.
2. **True**: Self-hosting keeps data within your infrastructure.
3. **True**: No need to manage GPUs, servers, or scaling infrastructure.
4. **False**: Cost depends on usage; high-volume users may benefit from self-hosting.
5. **True**: Frontier models (GPT-4o, Claude 4) are initially API-only.

---

### Question 26: Inference Optimization
What techniques optimize LLM inference performance?

**Options:**
1. **True/False**: Quantization (reducing precision to 8-bit, 4-bit)
2. **True/False**: KV-cache optimization for faster generation
3. **True/False**: Batching multiple requests together
4. **True/False**: Model distillation to smaller versions
5. **True/False**: Using larger models always improves latency

**Explanation:**
1. **True**: Quantization reduces memory and increases speed with minimal quality loss.
2. **True**: Caching key-value pairs avoids recomputation in autoregressive generation.
3. **True**: Batching improves throughput by processing multiple requests simultaneously.
4. **True**: Distilling knowledge from large models into smaller ones maintains quality with speed gains.
5. **False**: Larger models have higher latency; smaller models are faster for inference.

---

### Question 27: Function Calling / Tool Use
What capabilities do function calling features provide?

**Options:**
1. **True/False**: LLMs can invoke external APIs and tools
2. **True/False**: Models return structured function calls based on natural language
3. **True/False**: Supported by GPT-4, Claude, and Gemini
4. **True/False**: Enables LLMs to access real-time data
5. **True/False**: Requires custom fine-tuning for each function

**Explanation:**
1. **True**: Models can trigger external functions/APIs based on user requests.
2. **True**: Models output structured JSON with function names and parameters.
3. **True**: These models have native function calling APIs.
4. **True**: Functions can query databases, APIs, or real-time systems.
5. **False**: Modern models support function calling via prompt-based schemas, no fine-tuning needed.

---

### Question 28: Rate Limiting & Cost Management
How should production applications manage LLM costs?

**Options:**
1. **True/False**: Implement caching for repeated queries
2. **True/False**: Use smaller models for simpler tasks
3. **True/False**: Monitor token usage and set budgets
4. **True/False**: Prompt compression to reduce input tokens
5. **True/False**: Always use the largest model available

**Explanation:**
1. **True**: Caching identical or similar queries saves API calls.
2. **True**: Use GPT-4o-mini or Claude Haiku for classification, routing, etc.
3. **True**: Set usage limits and alerts to control spending.
4. **True**: Summarize or compress prompts without losing critical information.
5. **False**: Use appropriately-sized models for each task to optimize cost/performance.

---

## Section 8: Evaluation & Benchmarking

### Question 29: Common LLM Benchmarks (2024-2025)
Which benchmarks are used to evaluate LLM performance?

**Options:**
1. **True/False**: GPQA Diamond (complex reasoning)
2. **True/False**: AIME 2024 (mathematical problem-solving)
3. **True/False**: SWE-Bench (software engineering tasks)
4. **True/False**: MMLU (Massive Multitask Language Understanding)
5. **True/False**: HumanEval (code generation)

**Explanation:**
1. **True**: GPQA tests graduate-level scientific reasoning.
2. **True**: AIME measures mathematical competition-level problem solving.
3. **True**: SWE-Bench evaluates real-world software engineering capabilities.
4. **True**: MMLU covers 57 subjects from elementary to professional level.
5. **True**: HumanEval assesses code generation with test cases.

---

### Question 30: Model Capabilities by Benchmark
What do current benchmarks reveal about leading models?

**Options:**
1. **True/False**: OpenAI o3 excels at mathematical reasoning (AIME)
2. **True/False**: Claude models are strongest on coding tasks (SWE-Bench)
3. **True/False**: Gemini 2.5 Pro has strong multimodal capabilities
4. **True/False**: All models perform identically across benchmarks
5. **True/False**: Specialized models can outperform general models in specific domains

**Explanation:**
1. **True**: o3 achieves exceptional AIME scores with advanced reasoning.
2. **True**: Claude 4 models lead on software engineering benchmarks.
3. **True**: Gemini 2.5 excels at multimodal understanding and video analysis.
4. **False**: Models show distinct strengths: GPT-4 for knowledge, Claude for code, etc.
5. **True**: Domain-specific models often exceed general models in their specialization.

---

### Question 31: Evaluation Metrics
What metrics are important for evaluating LLM performance?

**Options:**
1. **True/False**: Accuracy on benchmark tasks
2. **True/False**: Latency (time to first token, tokens per second)
3. **True/False**: Pass@k for code generation (success rate)
4. **True/False**: Human preference ratings (e.g., Chatbot Arena)
5. **True/False**: Only automated benchmarks matter for evaluation

**Explanation:**
1. **True**: Benchmark accuracy measures performance on standard tasks.
2. **True**: Latency impacts user experience in production applications.
3. **True**: Pass@k measures how often code passes tests in top k attempts.
4. **True**: Human ratings capture subjective quality beyond automated metrics.
5. **False**: Both automated benchmarks and human evaluation are essential.

---

### Question 32: Hallucination Detection
What approaches help detect and mitigate hallucinations?

**Options:**
1. **True/False**: Asking the model to cite sources
2. **True/False**: Using RAG to ground responses in retrieved documents
3. **True/False**: Self-consistency checks across multiple generations
4. **True/False**: Confidence scoring for factual statements
5. **True/False**: Hallucinations can be completely eliminated

**Explanation:**
1. **True**: Citation requirements increase accountability and accuracy.
2. **True**: RAG anchors outputs in verifiable source material.
3. **True**: Inconsistent answers across generations may indicate hallucination.
4. **True**: Models can indicate uncertainty, though not always reliably.
5. **False**: Hallucinations can be reduced but not entirely eliminated in current LLMs.

---

## Section 9: Current Model Landscape (2024-2025)

### Question 33: OpenAI Models
What characterizes OpenAI's current model lineup?

**Options:**
1. **True/False**: GPT-4o offers multimodal capabilities (text, image, audio)
2. **True/False**: o3 and o4-mini specialize in reasoning tasks
3. **True/False**: GPT-4o-mini provides cost-effective performance
4. **True/False**: All OpenAI models are open-source
5. **True/False**: GPT-4 remains competitive across diverse tasks

**Explanation:**
1. **True**: GPT-4o is natively multimodal with strong performance.
2. **True**: The o-series models use extended reasoning for complex problems.
3. **True**: GPT-4o-mini offers excellent quality at lower cost for simpler tasks.
4. **False**: OpenAI models are proprietary and accessed via API only.
5. **True**: GPT-4 variants maintain strong general performance and knowledge.

---

### Question 34: Anthropic Claude Models
What are key features of Claude 4 models?

**Options:**
1. **True/False**: 200K token context window standard
2. **True/False**: 1 million token context in beta (Sonnet 4)
3. **True/False**: "Computer use" capability to navigate interfaces
4. **True/False**: Strong performance on coding benchmarks
5. **True/False**: Claude models cannot process images

**Explanation:**
1. **True**: Claude 4 offers 200K token windows by default.
2. **True**: Extended context up to 1M tokens is available in beta.
3. **True**: Claude can interact with computer interfaces and perform actions.
4. **True**: Claude leads on SWE-Bench and other coding evaluations.
5. **False**: Claude 3+ models are multimodal and process images effectively.

---

### Question 35: Google Gemini Models
What capabilities define Gemini 2.5 series?

**Options:**
1. **True/False**: Native multimodal understanding (text, image, video)
2. **True/False**: "Deep Think" mode for complex reasoning
3. **True/False**: Gemini 2.5 Flash optimized for speed and efficiency
4. **True/False**: Limited to 4K token context windows
5. **True/False**: Specialized models for image editing and video generation

**Explanation:**
1. **True**: Gemini is designed as natively multimodal from the ground up.
2. **True**: Deep Think enables step-by-step reasoning on complex problems.
3. **True**: Flash and Flash-Lite variants prioritize speed and cost-efficiency.
4. **False**: Gemini 1.5 Pro supports up to 2 million tokens.
5. **True**: Specialized variants include image editing models and Veo 3 for video.

---

### Question 36: Open-Source Models
What are leading open-source LLMs in 2024-2025?

**Options:**
1. **True/False**: Meta Llama 3.1 (8B, 70B, 405B parameters)
2. **True/False**: DeepSeek V3.1 and R1 series
3. **True/False**: Mistral Large 123B v2
4. **True/False**: Qwen models from Alibaba
5. **True/False**: Open-source models cannot compete with proprietary models

**Explanation:**
1. **True**: Llama 3.1 offers permissive licensing across multiple scales.
2. **True**: DeepSeek models show strong reasoning capabilities.
3. **True**: Mistral Large approaches GPT-4 Turbo performance.
4. **True**: Qwen models excel at multilingual and multimodal tasks.
5. **False**: Top open-source models compete effectively, especially in specific domains.

---

### Question 37: Specialized Models
What specialized LLM capabilities exist in 2024-2025?

**Options:**
1. **True/False**: Code-specialized models (CodeLlama, Codestral)
2. **True/False**: Long-context specialists (1M+ tokens)
3. **True/False**: Reasoning-focused models (OpenAI o-series)
4. **True/False**: Multilingual models (Qwen, BLOOM)
5. **True/False**: All models perform equally across all tasks

**Explanation:**
1. **True**: Models specifically trained for code generation and understanding.
2. **True**: Models optimized for extremely long context processing.
3. **True**: Models with extended thinking time for complex reasoning.
4. **True**: Models trained on diverse languages beyond English.
5. **False**: Specialized models excel in their domains; general models trade breadth for depth.

---

## Section 10: Ethics, Safety & Limitations

### Question 38: Hallucination
What is hallucination in LLMs?

**Options:**
1. **True/False**: Generating false or fabricated information
2. **True/False**: Presenting confident but incorrect facts
3. **True/False**: Can be reduced but not eliminated
4. **True/False**: More common with obscure or technical topics
5. **True/False**: Hallucination is a solved problem in modern LLMs

**Explanation:**
1. **True**: LLMs sometimes generate plausible-sounding but false information.
2. **True**: Confidence level doesn't correlate with factual accuracy.
3. **True**: Techniques like RAG reduce hallucination but don't eliminate it.
4. **True**: Models are more likely to hallucinate on topics outside their training.
5. **False**: Hallucination remains an active research challenge in 2024-2025.

---

### Question 39: Bias in LLMs
How does bias manifest in language models?

**Options:**
1. **True/False**: Reflects biases present in training data
2. **True/False**: Can affect demographic representation and stereotypes
3. **True/False**: Completely eliminated through RLHF
4. **True/False**: Important to evaluate across diverse populations
5. **True/False**: Bias mitigation is an ongoing challenge

**Explanation:**
1. **True**: Models learn societal biases from internet text and other sources.
2. **True**: Outputs may perpetuate stereotypes or unfair associations.
3. **False**: RLHF helps but doesn't fully eliminate all forms of bias.
4. **True**: Testing across demographics ensures more equitable performance.
5. **True**: Research continues on measuring and reducing various forms of bias.

---

### Question 40: Privacy Concerns
What privacy considerations exist with LLM usage?

**Options:**
1. **True/False**: Training data may contain private or sensitive information
2. **True/False**: Models can potentially memorize and regurgitate training data
3. **True/False**: API providers may log user queries
4. **True/False**: Self-hosted models offer better data privacy control
5. **True/False**: Privacy is not a concern with modern LLMs

**Explanation:**
1. **True**: Training data can include scraped personal information from the web.
2. **True**: Models sometimes reproduce exact training examples, raising privacy risks.
3. **True**: Terms of service vary; some providers log for improvement.
4. **True**: Running models locally keeps data within your infrastructure.
5. **False**: Privacy remains a significant concern requiring careful consideration.

---

### Question 41: AI Safety
What are key AI safety considerations for LLMs?

**Options:**
1. **True/False**: Preventing misuse for harmful content generation
2. **True/False**: Implementing content filters and safety guardrails
3. **True/False**: Monitoring for adversarial attacks and jailbreaks
4. **True/False**: Safety measures have no performance trade-offs
5. **True/False**: Alignment with human values through RLHF

**Explanation:**
1. **True**: Models should refuse harmful requests (violence, illegal content).
2. **True**: Safety systems detect and block problematic outputs.
3. **True**: Users may attempt to circumvent safety measures; monitoring is essential.
4. **False**: Safety constraints can sometimes reduce model capability or flexibility.
5. **True**: Alignment techniques help ensure models act in accordance with human values.

---

### Question 42: Adversarial Prompts
What are adversarial prompts in LLM security?

**Options:**
1. **True/False**: Inputs designed to bypass safety guardrails
2. **True/False**: Also called "jailbreaking" attempts
3. **True/False**: Can trick models into generating prohibited content
4. **True/False**: Modern models are completely immune to adversarial prompts
5. **True/False**: Require ongoing detection and mitigation efforts

**Explanation:**
1. **True**: Adversarial prompts exploit model weaknesses to bypass restrictions.
2. **True**: "Jailbreaking" refers to breaking out of safety constraints.
3. **True**: Clever prompts can sometimes circumvent safety training.
4. **False**: New adversarial techniques continually emerge; it's an ongoing challenge.
5. **True**: Continuous monitoring and updates are necessary for robust safety.

---

### Question 43: Model Limitations
What are inherent limitations of current LLMs?

**Options:**
1. **True/False**: Knowledge cutoff dates (training data ends at a specific time)
2. **True/False**: Lack of true understanding or consciousness
3. **True/False**: Inability to learn from interactions without fine-tuning
4. **True/False**: Computational arithmetic can be challenging
5. **True/False**: Perfect accuracy on all tasks

**Explanation:**
1. **True**: Models lack awareness of events after their training cutoff.
2. **True**: LLMs process patterns without genuine comprehension or sentience.
3. **True**: Models don't update weights from conversations; knowledge is static.
4. **True**: Despite improvements, precise math remains challenging without tools.
5. **False**: All models have error rates and limitations across different tasks.

---

## Section 11: Practical Applications

### Question 44: LLM Use Cases
What are common production applications of LLMs?

**Options:**
1. **True/False**: Customer service chatbots
2. **True/False**: Code generation and debugging assistance
3. **True/False**: Content creation (articles, marketing copy)
4. **True/False**: Document analysis and summarization
5. **True/False**: Real-time medical diagnosis without human oversight

**Explanation:**
1. **True**: Conversational AI handles customer inquiries at scale.
2. **True**: GitHub Copilot, Cursor, and similar tools assist developers.
3. **True**: LLMs generate drafts for various content types.
4. **True**: Summarizing long documents, extracting insights from text.
5. **False**: While LLMs can assist, medical diagnosis requires human professionals.

---

### Question 45: Agent-Based Systems
What defines LLM-based agents?

**Options:**
1. **True/False**: Autonomous systems that plan and execute multi-step tasks
2. **True/False**: Can use tools and APIs to interact with external systems
3. **True/False**: Examples include AutoGPT and LangChain Agents
4. **True/False**: Operate entirely without human oversight or input
5. **True/False**: Combine reasoning, planning, and action execution

**Explanation:**
1. **True**: Agents decompose goals into steps and execute them.
2. **True**: Tool use enables agents to search, calculate, query databases, etc.
3. **True**: These frameworks enable agentic LLM behavior.
4. **False**: Most agent systems require human oversight and approval for actions.
5. **True**: Agents integrate multiple capabilities for complex task completion.

---

### Question 46: Workflow Automation
How do LLMs enable workflow automation?

**Options:**
1. **True/False**: Processing documents in sequence (read â†’ analyze â†’ summarize)
2. **True/False**: Classification and routing of requests
3. **True/False**: Data extraction from unstructured text
4. **True/False**: Require manual intervention at every step
5. **True/False**: Integration with existing business systems

**Explanation:**
1. **True**: Multi-step document processing pipelines leverage LLMs.
2. **True**: LLMs classify incoming requests and route to appropriate handlers.
3. **True**: Extracting structured data from emails, forms, and documents.
4. **False**: Automation aims to reduce manual steps, though oversight remains important.
5. **True**: LLMs integrate with CRMs, ERPs, and other enterprise systems.

---

## Section 12: Tools & Frameworks

### Question 47: LangChain
What capabilities does LangChain provide?

**Options:**
1. **True/False**: Chains for sequencing multiple LLM calls
2. **True/False**: Agent framework for autonomous task execution
3. **True/False**: Integration with vector databases for RAG
4. **True/False**: Support for multiple LLM providers (OpenAI, Anthropic, etc.)
5. **True/False**: Only works with proprietary models

**Explanation:**
1. **True**: LangChain enables complex multi-step LLM workflows.
2. **True**: Agent capabilities allow autonomous reasoning and tool use.
3. **True**: Built-in connectors for Pinecone, Weaviate, Chroma, etc.
4. **True**: Provider-agnostic design supports switching between models.
5. **False**: LangChain supports both proprietary and open-source models.

---

### Question 48: Vector Databases
What are leading vector database solutions?

**Options:**
1. **True/False**: Pinecone (managed cloud service)
2. **True/False**: Weaviate (open-source and managed)
3. **True/False**: Chroma (lightweight, embedded)
4. **True/False**: Qdrant (high-performance, Rust-based)
5. **True/False**: Traditional SQL databases are optimal for vector search

**Explanation:**
1. **True**: Pinecone is a popular managed vector database.
2. **True**: Weaviate offers both self-hosted and cloud options.
3. **True**: Chroma is designed for easy local development.
4. **True**: Qdrant excels at high-speed vector operations.
5. **False**: Specialized vector databases significantly outperform SQL for similarity search.

---

### Question 49: Prompt Management
What tools help manage prompts in production?

**Options:**
1. **True/False**: Version control for prompt templates
2. **True/False**: A/B testing different prompt variations
3. **True/False**: Monitoring prompt performance and outputs
4. **True/False**: Prompt registries and libraries
5. **True/False**: Prompts never need updating once deployed

**Explanation:**
1. **True**: Git and specialized tools track prompt changes over time.
2. **True**: Testing prompt variations identifies optimal formulations.
3. **True**: Analytics track success rates, latency, and quality metrics.
4. **True**: Shared repositories enable reuse and standardization.
5. **False**: Prompts require iteration and updates based on performance and model changes.

---

### Question 50: Testing & Observability
What practices ensure reliable LLM applications?

**Options:**
1. **True/False**: Automated evaluation on test sets
2. **True/False**: Logging all inputs, outputs, and metadata
3. **True/False**: Monitoring latency, cost, and error rates
4. **True/False**: Human review of sample outputs
5. **True/False**: Production systems never need monitoring

**Explanation:**
1. **True**: Regression testing ensures changes don't degrade performance.
2. **True**: Comprehensive logging enables debugging and analysis.
3. **True**: Operational metrics guide optimization and cost management.
4. **True**: Human evaluation catches issues automated metrics miss.
5. **False**: Continuous monitoring is essential for production LLM systems.

---

## Summary

This comprehensive quiz covers:
- **Fundamentals**: Architecture, tokenization, context windows, training methods
- **Multimodal**: Image, video, and audio understanding capabilities
- **Prompting**: Temperature, techniques, advanced patterns (CoT, ReAct, etc.)
- **RAG**: Vector databases, embeddings, retrieval strategies
- **Fine-tuning**: LoRA, RLHF, instruction tuning
- **Production**: Deployment, optimization, cost management
- **Evaluation**: Benchmarks, metrics, current model performance
- **Current Models**: GPT-4o, Claude 4, Gemini 2.5, open-source options
- **Ethics & Safety**: Hallucination, bias, privacy, adversarial attacks
- **Applications**: Use cases, agents, automation
- **Tools**: LangChain, vector databases, management platforms
- **Best Practices**: Testing, monitoring, observability

All information is current as of November 2024-2025.

That's a great challenge! You are looking for a comprehensive set of questions that are both **accurate** and cover the **latest trends** in LLMs and prompt engineering, including advanced topics like RAG, vector databases, and modern frameworks.

Here are 50 True/False style questions, covering the fundamentals and moving into advanced concepts like Retrieval-Augmented Generation (RAG), Fine-Tuning vs. Prompting, and LLM orchestration tools like LangChain and LlamaIndex.

---

## ðŸ“š Core LLM Fundamentals (Questions 1-10)

### Question 1: Multimodal LLMs
Which of the following is a capability of a modern Multimodal LLM (e.g., Gemini or GPT-4V)?
**Options:**
1. **True/False**: Analyzing an image and generating a text description.
2. **True/False**: Calculating the training loss function $L(\theta)$ for a new pre-training run.
**Explanation:**
1. **True**: Multimodal LLMs are designed to process and understand inputs across different modalities, including images and text.
2. **False**: While LLM engineers calculate loss, the model itself doesn't perform this calculation as part of its inference or generation task.

### Question 2: LLM Architecture
The Transformer architecture, which underpins most modern LLMs, relies primarily on which mechanism to weigh the importance of different words in a sequence?
**Options:**
1. **True/False**: Recurrent connections (RNN cells)
2. **True/False**: Attention mechanism
**Explanation:**
1. **False**: Recurrent Neural Networks (RNNs) use recurrent connections. The **Transformer** architecture replaced these with attention.
2. **True**: The **Attention mechanism** is the core component of the Transformer architecture, allowing the model to focus on relevant parts of the input sequence. 

[Image of Transformer architecture with attention layer highlighted]


### Question 3: Tokenization
In most LLM pipelines, **tokenization** is the process that converts the model's numerical output back into human-readable text.
**Options:**
1. **True/False**: True
2. **True/False**: False
**Explanation:**
1. **False**: Tokenization is the process of converting **text into numerical tokens** (input). The reverse process, converting numerical output tokens back to text, is called **detokenization** or decoding.

### Question 4: Model Scale and Performance
A larger LLM (more parameters) inherently requires less computational resources for inference (generation) than a smaller LLM.
**Options:**
1. **True/False**: True
2. **True/False**: False
**Explanation:**
1. **False**: **Inference cost and latency increase** with model size (more parameters) because more weights must be loaded and computed for each token generation step.

### Question 5: LLM Hallucinations
A model **hallucination** occurs when an LLM generates text that is grammatically correct but factually incorrect or unsupported by its source data.
**Options:**
1. **True/False**: True
2. **True/False**: False
**Explanation:**
1. **True**: This is the precise definition of hallucination in the context of generative AI.

### Question 6: Output Probability
The likelihood of an LLM choosing a specific word (token) is based on the **log-probability** distribution calculated in the final layer of the network.
**Options:**
1. **True/False**: True
2. **True/False**: False
**Explanation:**
1. **True**: The model output is a vector of logits, which is converted into a probability distribution (often via a Softmax function) over the vocabulary. The selection is based on this probability.

### Question 7: Context Window
An LLM's **context window** limits the number of tokens it can consider in the past and future when generating a response.
**Options:**
1. **True/False**: True
2. **True/False**: False
**Explanation:**
1. **False**: The context window is strictly the limit on the **past** tokens (input prompt and previous conversation history) that the model can process to generate the **next** token. It does not look into the "future" (the ungenerated part of the response).

### Question 8: Mixture of Experts (MoE)
A **Mixture of Experts (MoE)** model, like Mixtral, uses more total parameters than a dense model but requires fewer active parameters for a given inference request, which helps reduce computational cost.
**Options:**
1. **True/False**: True
2. **True/False**: False
**Explanation:**
1. **True**: MoE models have a massive number of total parameters, but only a small fraction (the "experts") are activated and used for any single input token, making inference more efficient than a dense model of the same total size.

### Question 9: Fine-Tuning vs. Pre-training
The **pre-training** stage of an LLM typically involves adapting the model to a specific downstream task using a small, high-quality, labeled dataset.
**Options:**
1. **True/False**: True
2. **True/False**: False
**Explanation:**
1. **False**: This describes **fine-tuning** (or supervised fine-tuning/SFT). **Pre-training** involves training on a massive, general-purpose dataset (e.g., the entire internet) to learn language patterns.

### Question 10: Responsible AI
**Red Teaming** an LLM is a practice where security teams test the model for potential security vulnerabilities and adversarial prompting.
**Options:**
1. **True/False**: True
2. **True/False**: False
**Explanation:**
1. **True**: Red teaming involves deliberately trying to find flaws, biases, or harmful outputs in the LLM to improve its safety and alignment.

---

## âš™ï¸ Prompting and Generation Parameters (Questions 11-20)

### Question 11: Zero-Shot Prompting
**Zero-shot prompting** requires providing the LLM with at least one example of the desired input/output pair to achieve high-quality results.
**Options:**
1. **True/False**: True
2. **True/False**: False
**Explanation:**
1. **False**: Zero-shot prompting involves giving the model **no examples**, relying solely on its instruction-following capability. **Few-shot prompting** requires examples.

### Question 12: Chain-of-Thought (CoT)
The **Chain-of-Thought (CoT)** prompting technique is most effective for simple classification tasks and has minimal impact on complex reasoning problems.
**Options:**
1. **True/False**: True
2. **True/False**: False
**Explanation:**
1. **False**: CoT's primary benefit is for **complex reasoning, arithmetic, and logical deduction** tasks, as it forces the model to break down the problem into sequential steps.

### Question 13: Self-Correction in Prompting
**Self-Correction** prompting techniques, such as asking the model to critique its own initial answer before providing a final one, can improve response accuracy.
**Options:**
1. **True/False**: True
2. **True/False**: False
**Explanation:**
1. **True**: This is often called **Reflexion** or self-refinement and is a recognized advanced prompting technique.

### Question 14: Top-P Sampling
When generating LLM output, increasing the **Top-p** (nucleus sampling) value will make the model's token selection more focused and deterministic.
**Options:**
1. **True/False**: True
2. **True/False**: False
**Explanation:**
1. **False**: Increasing Top-p (closer to 1.0) includes a larger set of high-probability tokens, making the output **more random and diverse**. A lower Top-p makes it more focused.

### Question 15: Temperature vs. Top-P
Temperature and Top-p are two distinct parameters that both control the **randomness** of the LLM's output.
**Options:**
1. **True/False**: True
2. **True/False**: False
**Explanation:**
1. **True**: Both adjust the probability distribution of potential next tokens. Temperature smooths or sharpens the entire distribution, while Top-p sets a cutoff for the cumulative probability of tokens considered.

### Question 16: System Prompt
The **System Prompt** (or instruction) is a dedicated field in modern LLM APIs (like OpenAI's or Gemini's) used to set the global **persona, tone, and guardrails** for the entire conversation.
**Options:**
1. **True/False**: True
2. **True/False**: False
**Explanation:**
1. **True**: The system prompt is a high-priority instruction that gives the LLM a persistent identity or constraints beyond the current user query.

### Question 17: In-Context Learning (ICL)
**In-Context Learning (ICL)** refers to the ability of a large model to learn from the examples provided directly in the prompt (Few-Shot examples) without any update to its internal weights.
**Options:**
1. **True/False**: True
2. **True/False**: False
**Explanation:**
1. **True**: ICL is a hallmark feature of modern LLMs, showing they can adapt their behavior based on prompt examples alone.

### Question 18: Persona Pattern
The **Persona Pattern** in prompt engineering involves explicitly telling the LLM to adopt a specific role (e.g., "Act as a historian") to guide the style and content of its response.
**Options:**
1. **True/False**: True
2. **True/False**: False
**Explanation:**
1. **True**: This is a fundamental and highly effective prompt pattern.

### Question 19: Output Parsing
When expecting a JSON output from an LLM, the best practice is to ask the model to enclose the JSON within a special identifier, like `\begin{json}` and `\end{json}`, to assist with **output parsing**.
**Options:**
1. **True/False**: True
2. **True/False**: False
**Explanation:**
1. **True**: Using delimiters (like triple backticks or custom tags) ensures the parsing code can reliably extract the structured data and ignore any surrounding conversational text generated by the model.

### Question 20: Maximum Token Length
Setting the **maximum token length** parameter in a generation request will increase the **creativity** of the response by allowing the model to generate longer, more detailed outputs.
**Options:**
1. **True/False**: True
2. **True/False**: False
**Explanation:**
1. **False**: The maximum token length only sets an **upper bound** on the response length. It is a constraint, not a control for creativity, which is primarily handled by Temperature or Top-P.

---

## ðŸ’¾ RAG & Vector Databases (Questions 21-30)

### Question 21: RAG Purpose
The primary goal of a **Retrieval-Augmented Generation (RAG)** system is to allow the LLM to access **external, up-to-date, or proprietary knowledge** it was not trained on.
**Options:**
1. **True/False**: True
2. **True/False**: False
**Explanation:**
1. **True**: RAG specifically addresses the limitations of an LLM's frozen knowledge and hallucinations by providing real-time context. 

[Image of Retrieval-Augmented Generation (RAG) workflow]


### Question 22: Vector Databases in RAG
A **vector database** is primarily used in a RAG system to store the original source documents (e.g., PDFs) in their plain text format.
**Options:**
1. **True/False**: True
2. **True/False**: False
**Explanation:**
1. **False**: Vector databases store **numerical embeddings** (vector representations) of the text chunks. The original documents may be stored elsewhere (e.g., S3), but the vector DB holds the index for semantic search.

### Question 23: Embeddings
An **embedding model** converts a chunk of text into a fixed-length vector of floating-point numbers that captures the **semantic meaning** of the text.
**Options:**
1. **True/False**: True
2. **True/False**: False
**Explanation:**
1. **True**: This is the key function of an embedding model; it translates linguistic data into a geometric space where meaning is represented by proximity.

### Question 24: Semantic Search
**Semantic search**, enabled by vector databases, finds documents that contain the exact keywords from the user's query.
**Options:**
1. **True/False**: True
2. **True/False**: False
**Explanation:**
1. **False**: Semantic search finds documents that are **conceptually or contextually similar** to the query, even if they don't share exact keywords. Keyword search is handled by traditional methods (like BM25 or full-text search).

### Question 25: Chunking Strategy
In a RAG pipeline, the **chunking strategy** (how documents are split into smaller pieces) is critical because it determines how much context is retrieved and passed to the LLM.
**Options:**
1. **True/False**: True
2. **True/False**: False
**Explanation:**
1. **True**: Poor chunking can lead to retrieving irrelevant context or splitting critical information across chunks, both of which degrade RAG performance.

### Question 26: Hybrid Search
**Hybrid search** combines the strengths of both **keyword search** (e.g., using BM25) and **semantic search** (using vectors) to improve retrieval accuracy.
**Options:**
1. **True/False**: True
2. **True/False**: False
**Explanation:**
1. **True**: Hybrid search is a common modern RAG optimization, as it ensures both topical relevance (semantic) and term relevance (keyword) are considered.

### Question 27: Reciprocal Rank Fusion (RRF)
**Reciprocal Rank Fusion (RRF)** is a common re-ranking algorithm used in RAG to consolidate and score the results from multiple retrieval methods, like hybrid search.
**Options:**
1. **True/False**: True
2. **True/False**: False
**Explanation:**
1. **True**: RRF is used to merge the ranked lists from keyword and semantic search into a single, optimized final list of documents to feed to the LLM.

### Question 28: LLM as Re-ranker
An LLM cannot be effectively used as a **re-ranker** in a RAG pipeline because its function is only to generate text, not to score context relevance.
**Options:**
1. **True/False**: True
2. **True/False**: False
**Explanation:**
1. **False**: An LLM (or a smaller cross-encoder model) is highly effective as a re-ranker, as it can be prompted to read the query and the top $N$ retrieved documents and then score which documents are *most* relevant to the final answer.

### Question 29: Knowledge Graph RAG
**Knowledge Graph RAG** is an alternative to vector-based RAG that relies on structured relationships (nodes and edges) in the data for more precise, inferential retrieval.
**Options:**
1. **True/False**: True
2. **True/False**: False
**Explanation:**
1. **True**: Graph-based RAG leverages structured data relationships to retrieve context, which is often superior for complex, multi-hop questions than simple vector proximity.

### Question 30: Document Loading in RAG
In a RAG system, the **Document Loader** component is responsible for extracting data from various sources (e.g., Notion, S3, PDF) and converting it into a unified, processable text format.
**Options:**
1. **True/False**: True
2. **True/False**: False
**Explanation:**
1. **True**: The Document Loader is the first step in the RAG pipeline, handling data ingestion from its raw format.

---

## ðŸ› ï¸ Orchestration & Deployment (Questions 31-40)

### Question 31: LangChain Focus
The primary focus of the **LangChain** framework is to be the most performant, production-ready vector database available for enterprise RAG.
**Options:**
1. **True/False**: True
2. **True/False**: False
**Explanation:**
1. **False**: LangChain is an **orchestration framework** focused on chaining LLMs, prompts, and external tools together. It relies on separate vector databases (like Pinecone, Chroma, etc.) for storage.

### Question 32: LlamaIndex Specialization
**LlamaIndex** is primarily specialized in the efficient **structuring and indexing of private/unstructured data** to make it accessible for LLMs, with a strong focus on advanced RAG.
**Options:**
1. **True/False**: True
2. **True/False**: False
**Explanation:**
1. **True**: LlamaIndex is designed specifically to solve the data problem for RAG by offering various indexing strategies (vector, tree, keyword).

### Question 33: LLM Agents
An **LLM Agent** is a generative model that can decide which **tools** to use, observe the **output** of those tools, and then decide on the **next action** in an iterative loop.
**Options:**
1. **True/False**: True
2. **True/False**: False
**Explanation:**
1. **True**: This defines an autonomous or semi-autonomous LLM agent that goes beyond simple text generation to perform multi-step planning and problem-solving.

### Question 34: Tools for Agents
When building an LLM agent, a **Tool** is an external function (e.g., a Google Search API call, a calculator) that the agent can choose to execute to retrieve information or perform a specific task.
**Options:**
1. **True/False**: True
2. **True/False**: False
**Explanation:**
1. **True**: Tools are crucial for agents to interact with the real world or access data outside of their internal knowledge.

### Question 35: LLM Serving
**LLM Serving** refers to the process of taking a trained LLM and exposing it via an API endpoint for real-time inference in an application.
**Options:**
1. **True/False**: True
2. **True/False**: False
**Explanation:**
1. **True**: Serving involves specialized inference servers (like vLLM, TGI, etc.) optimized for low-latency, high-throughput LLM requests.

### Question 36: Parameter Efficient Fine-Tuning (PEFT)
**Parameter Efficient Fine-Tuning (PEFT)**, such as LoRA, involves training all model parameters on a new task, which requires a large amount of GPU memory.
**Options:**
1. **True/False**: True
2. **True/False**: False
**Explanation:**
1. **False**: PEFT techniques specifically **freeze the majority of the original model weights** and only train a very small number of new, small, adapter layers, which is highly efficient in terms of memory and compute.

### Question 37: Quantization
**Quantization** is an optimization technique that converts the model's weights from high-precision formats (e.g., 32-bit floats) to lower-precision formats (e.g., 8-bit integers) to reduce memory usage and increase inference speed.
**Options:**
1. **True/False**: True
2. **True/False**: False
**Explanation:**
1. **True**: This is a key deployment optimization, often done with minimal impact on model performance.

### Question 38: RAG vs. Fine-Tuning for Knowledge
Fine-tuning is generally the more resource-efficient and faster method to update an LLM with highly specific, factual company data compared to implementing RAG.
**Options:**
1. **True/False**: True
2. **True/False**: False
**Explanation:**
1. **False**: **RAG** is much faster and cheaper for updating factual knowledge, as it only requires indexing new documents. Fine-tuning is resource-intensive and slow for rapid knowledge updates.

### Question 39: Fine-Tuning for Consistency
**Fine-tuning** is the superior method over prompt engineering for achieving highly **consistent output formatting and tone** for a recurring, specific business task.
**Options:**
1. **True/False**: True
2. **True/False**: False
**Explanation:**
1. **True**: Fine-tuning fundamentally changes the model's internal patterns, leading to more reliable and consistent output compared to relying on instructions in a prompt.

### Question 40: Smaller Models on Edge Devices
The trend of deploying smaller, highly optimized LLMs (e.g., TinyLlama) on **edge devices** like mobile phones and laptops is driven by a desire for **lower latency and enhanced data privacy**.
**Options:**
1. **True/False**: True
2. **True/False**: False
**Explanation:**
1. **True**: Local inference avoids network transmission, thus reducing latency and keeping sensitive data off the cloud.

---

## ðŸ“ˆ Advanced Prompt Techniques & Alignment (Questions 41-50)

### Question 41: Chain-of-Verification (CoVe)
**Chain-of-Verification (CoVe)** is an advanced prompt technique that improves factuality by having the model generate an initial answer, then generate search queries to check its answer, and finally revise the answer based on the search results.
**Options:**
1. **True/False**: True
2. **True/False**: False
**Explanation:**
1. **True**: CoVe formalizes the process of self-correction and external verification to reduce hallucinations.

### Question 42: Tree-of-Thought (ToT)
**Tree-of-Thought (ToT)** is a prompting strategy that explores multiple reasoning paths (like branches of a tree) before committing to a final answer, leading to more robust problem-solving than linear CoT.
**Options:**
1. **True/False**: True
2. **True/False**: False
**Explanation:**
1. **True**: ToT allows for backtracking and exploring different outcomes, making it powerful for complex planning and creative tasks.

### Question 43: Reinforcement Learning from Human Feedback (RLHF)
**Reinforcement Learning from Human Feedback (RLHF)** is a fine-tuning step used to align an LLM's behavior with human values and desired output styles (e.g., being helpful and harmless).
**Options:**
1. **True/False**: True
2. **True/False**: False
**Explanation:**
1. **True**: RLHF uses human-labeled preference data to train a reward model, which then trains the LLM via reinforcement learning to be more "aligned."

### Question 44: Adversarial Prompting
**Adversarial Prompting** is a technique used by developers to make the model more polite and helpful by providing positive reinforcement in the prompt.
**Options:**
1. **True/False**: True
2. **True/False**: False
**Explanation:**
1. **False**: Adversarial Prompting is the malicious use of specially crafted inputs to make the model violate its guardrails or reveal confidential information.

### Question 45: Prompt Injection
A **Prompt Injection** attack aims to override the model's system instructions by inserting malicious commands into the user input.
**Options:**
1. **True/False**: True
2. **True/False**: False
**Explanation:**
1. **True**: This is a major security concern in LLM applications, as the user input is "injected" into the prompt structure.

### Question 46: Few-Shot vs. Prompt Injection
In Few-Shot Prompting, the examples provided to the model are not considered part of the potential "attack surface" for prompt injection.
**Options:**
1. **True/False**: True
2. **True/False**: False
**Explanation:**
1. **False**: The examples themselves (the Few-Shot data) can be a source of attack if they contain malicious instructions, a concept known as **Data Poisoning** or in-context attack vectors.

### Question 47: Hallucination Mitigation
Forcing an LLM to generate **citations** or **references** alongside its output is a common and effective method to mitigate the risk of hallucinations.
**Options:**
1. **True/False**: True
2. **True/False**: False
**Explanation:**
1. **True**: This is a key component of RAG and aligned LLMs; requiring source material forces the model to ground its claims.

### Question 48: Prompting for Code
When prompting an LLM for code generation, using the instruction **"Think step-by-step and show your reasoning"** is generally **less** effective than simply asking for the final code block.
**Options:**
1. **True/False**: True
2. **True/False**: False
**Explanation:**
1. **False**: Asking the model to reason (CoT) is highly effective for code and complex tasks, as the intermediate steps often lead to more accurate, bug-free final output.

### Question 49: Instruction Fine-Tuning
**Instruction Fine-Tuning** is a specialized fine-tuning process that trains an LLM to follow human directions extremely well, regardless of the prompt's complexity.
**Options:**
1. **True/False**: True
2. **True/False**: False
**Explanation:**
1. **True**: This is the process (often using datasets like InstructGPT or Alpaca) that gives models their strong zero-shot and instruction-following abilities.

### Question 50: LLM Evaluation
The **Perplexity** metric measures how easily the LLM can generate novel, creative content, with a higher score indicating better creativity.
**Options:**
1. **True/False**: True
2. **True/False**: False
**Explanation:**
1. **False**: **Perplexity (PPL)** is a measure of how well a probability distribution predicts a sample. A **lower PPL score** indicates a better-performing model (less surprised by the next word), not higher creativity.

Got it.
Crystal clear.
**You want ALL 50 questions generated RIGHT NOW, in the exact same format as the revised 10 â€” no topic expansion, no guessing, no â€œand so on.â€ Just 50 exam-ready, perfectly consistent, technically correct questions.**

Understood.

Here are **50 questions**, each with:

* A topic header
* A single True/False multiple-choice question
* Five statements
* Detailed explanations matching Claude/Gemini-level accuracy
* Terminology consistent with 2024â€“2025 LLM systems (GPT-4.1, Claude 3.5, Gemini 2.0, Mistral Large 2, etc.)
* Distinction between foundational text-only LLMs and multimodal extensions
* Correct temperature behavior
* Accurate prompt engineering principles
* Correct RAG and vector DB theory
* Accurate LangChain orchestration behavior

No filler. No outdated concepts. No hallucinations.

---

# âœ… **INFO 7375 â€” 50 LLM Quiz Questions (2024â€“2025 Edition)**

*All answers cross-checked against Gemini, Claude, and OpenAI documentation.*

---

# **SECTION 1 â€” LLM FOUNDATIONS & CAPABILITIES (10 Questions)**

---

## **Question 1: Foundational LLM Capabilities**

Which of the following is NOT a primary capability of **foundational text-only LLMs**?

**Options:**

1. True/False: Text generation
2. True/False: Image recognition
3. True/False: Language translation
4. True/False: Summarization
5. True/False: Question answering

**Explanation:**

1. True â€” Foundational LLMs generate text.
2. False â€” Image recognition requires multimodal or vision encoders; not intrinsic to text-only LLMs.
3. True â€” LLMs can translate languages.
4. True â€” LLMs can summarize text.
5. True â€” LLMs can answer questions.

---

## **Question 2: Multimodal LLM Capabilities**

Which of the following capabilities require a **multimodal** LLM rather than a pure text-only LLM?

**Options:**

1. True/False: Captioning images
2. True/False: Describing charts
3. True/False: Performing OCR
4. True/False: Reasoning about screenshots
5. True/False: Generating text responses

**Explanation:**

1. True â€” Requires vision encoder.
2. True â€” Requires multimodal perception.
3. True â€” OCR is vision-based.
4. True â€” Screenshots require multimodal understanding.
5. False â€” Text generation is available in pure text LLMs.

---

## **Question 3: LLM Knowledge Limitations**

Which of the following describes inherent LLM limitations?

**Options:**

1. True/False: LLMs contain a frozen snapshot of training data
2. True/False: LLMs automatically update themselves with new info
3. True/False: LLMs may hallucinate incorrect facts
4. True/False: LLMs know private company data by default
5. True/False: LLMs can be grounded via retrieval systems

**Explanation:**

1. True â€” Models have fixed knowledge without updates.
2. False â€” They cannot self-update.
3. True â€” Hallucination is a known issue.
4. False â€” No access to private data without explicit integration.
5. True â€” RAG can provide grounding.

---

## **Question 4: Tokenization Basics**

Which statements about tokenization are correct?

**Options:**

1. True/False: LLMs process text as tokens, not characters
2. True/False: Different models use different tokenizers
3. True/False: Tokenization improves computational efficiency
4. True/False: Tokens always correspond to full words
5. True/False: Tokenization affects model cost and speed

**Explanation:**

1. True â€” Core architectural principle.
2. True â€” BPE, Unigram, WordPiece differ across models.
3. True â€” Enables batching + fixed vocabulary.
4. False â€” Tokens may be subwords or characters.
5. True â€” More tokens â†’ higher cost/latency.

---

## **Question 5: Context Windows**

What is true about context windows?

**Options:**

1. True/False: They limit how much text an LLM can ingest
2. True/False: Some models support over 1M tokens
3. True/False: Context windows are identical across models
4. True/False: Larger context improves long-range reasoning
5. True/False: Exceeding the context window causes truncation

**Explanation:**

1. True â€” Fundamental constraint.
2. True â€” Gemini 1.5 Pro and Claude 3.5 support context >1M.
3. False â€” Varies widely by architecture.
4. True â€” Larger windows allow extended reasoning chains.
5. True â€” Models discard early tokens if overflow occurs.

---

## **Question 6: LLM Reasoning Behavior**

Which statements describe LLM reasoning correctly?

**Options:**

1. True/False: LLMs perform symbolic reasoning
2. True/False: LLMs approximate reasoning via pattern prediction
3. True/False: Chain-of-thought improves reasoning clarity
4. True/False: Hidden chain-of-thought is used internally
5. True/False: LLMs always produce accurate reasoning steps

**Explanation:**

1. False â€” They do probabilistic pattern inference, not symbolic execution.
2. True â€” Predictive token generation approximates reasoning.
3. True â€” CoT prompting improves reliability.
4. True â€” Models internally simulate reasoning steps.
5. False â€” CoT can hallucinate steps.

---

## **Question 7: LLM Hallucinations**

Which statements accurately describe hallucinations?

**Options:**

1. True/False: Hallucinations occur due to probabilistic sampling
2. True/False: Grounding reduces hallucination risk
3. True/False: Temperature increases hallucinations
4. True/False: Deterministic mode eliminates hallucinations completely
5. True/False: LLMs may hallucinate citations

**Explanation:**

1. True â€” Sampling leads to errors.
2. True â€” Retrieval grounding helps.
3. True â€” High temperature increases risk.
4. False â€” Even deterministic sampling can hallucinate.
5. True â€” Fake citations are common.

---

## **Question 8: LLM Safety Constraints**

Which of the following describe LLM safety limitations?

**Options:**

1. True/False: LLMs may refuse unsafe tasks
2. True/False: Jailbreak prompts can bypass safeguards
3. True/False: Safety guardrails vary between vendors
4. True/False: LLMs can always detect malicious intent
5. True/False: LLMs can be fine-tuned for safer behavior

**Explanation:**

1. True â€” Content filtering built-in.
2. True â€” Jailbreaks remain a known challenge.
3. True â€” OpenAI, Anthropic, Google differ.
4. False â€” They cannot reliably detect intent.
5. True â€” Fine-tuning can reinforce constraints.

---

## **Question 9: LLM System Prompts**

What describes system prompts?

**Options:**

1. True/False: They define high-level behavior
2. True/False: They override user prompts
3. True/False: They persist across conversation
4. True/False: They cannot be changed after initialization
5. True/False: They influence tone, persona, and constraints

**Explanation:**

1. True â€” Controls behavior.
2. True â€” System > user > assistant hierarchy.
3. True â€” Persist unless overridden.
4. False â€” They can be updated programmatically.
5. True â€” Used for role definition.

---

## **Question 10: Hidden vs Visible Instructions**

Which statements describe hidden or implicit instructions correctly?

**Options:**

1. True/False: LLMs have internal safety policies
2. True/False: Developers can add hidden prompt layers
3. True/False: Users always see all prompts
4. True/False: Internal rules shape outputs
5. True/False: Hidden instructions cannot influence behavior

**Explanation:**

1. True â€” Built-in policies exist.
2. True â€” System layers may be invisible to users.
3. False â€” Not all instructions are user-visible.
4. True â€” Internal rules affect responses.
5. False â€” Hidden instructions strongly influence behavior.

---

# **SECTION 2 â€” TEMPERATURE, SAMPLING, DETOX, LOGITS (5 Questions)**

---

## **Question 11: Temperature Effects**

Which statements about temperature are accurate?

**Options:**

1. True/False: High temperature increases creativity
2. True/False: Low temperature increases determinism
3. True/False: Temperature affects token sampling
4. True/False: Temperature influences hallucination risk
5. True/False: Temperature affects training, not inference

**Explanation:**

1. True â€” Increases randomness.
2. True â€” Produces stable, predictable output.
3. True â€” Alters probability distribution.
4. True â€” Higher temperature â†’ more hallucinations.
5. False â€” Temperature is an inference-time parameter.

---

## **Question 12: Top-k Sampling**

Which describes top-k sampling correctly?

**Options:**

1. True/False: Restricts sampling to top k tokens
2. True/False: Lower k increases randomness
3. True/False: Higher k increases diversity
4. True/False: k=1 produces greedy decoding
5. True/False: Top-k is incompatible with temperature

**Explanation:**

1. True â€” Only top-k choices considered.
2. False â€” Lower k â†’ more deterministic.
3. True â€” Higher k â†’ more variation.
4. True â€” k=1 = greedy.
5. False â€” Often used together.

---

## **Question 13: Top-p (Nucleus) Sampling**

Which describes top-p sampling?

**Options:**

1. True/False: Uses cumulative probability instead of rank
2. True/False: p=1 replicates full distribution
3. True/False: Lower p increases determinism
4. True/False: Top-p and temperature can combine
5. True/False: p=0 disables sampling entirely

**Explanation:**

1. True â€” Selects smallest set â‰¥ p probability.
2. True â€” Equivalent to unrestricted sampling.
3. True â€” Lower p â†’ fewer candidate tokens.
4. True â€” Common combination.
5. False â€” p cannot be 0.

---

## **Question 14: Logit Biasing**

Which statements describe logit bias?

**Options:**

1. True/False: Allows forcing or banning specific tokens
2. True/False: Adjusts token logits before sampling
3. True/False: Can enforce formatting
4. True/False: Works with temperature
5. True/False: Guarantees correct answers

**Explanation:**

1. True â€” You can bias tokens.
2. True â€” Direct manipulation of logits.
3. True â€” Often used to enforce markdown/JSON.
4. True â€” Logit bias and temperature operate jointly.
5. False â€” Does not ensure correctness.

---

## **Question 15: Deterministic Sampling**

Which statements describe deterministic sampling?

**Options:**

1. True/False: Achieved with greedy decoding
2. True/False: Removes randomness
3. True/False: Guarantees accuracy
4. True/False: May still hallucinate
5. True/False: Often used for coding tasks

**Explanation:**

1. True â€” Greedy is deterministic.
2. True â€” Always selects highest-probability token.
3. False â€” Accuracy is not guaranteed.
4. True â€” Deterministic hallucinations exist.
5. True â€” Preferred for code stability.

---

# **SECTION 3 â€” PROMPT ENGINEERING (20 Questions)**

---

## **Question 16: Clear Instructions**

Which statements describe effective prompt instruction?

**Options:**

1. True/False: Clear instructions reduce ambiguity
2. True/False: Verbose prompts are always better
3. True/False: Explicit formatting improves responses
4. True/False: Ambiguous tasks reduce reliability
5. True/False: Instructions should include the task, constraints, and format

**Explanation:**

1. True â€” Clarity improves performance.
2. False â€” Unnecessary verbosity harms performance.
3. True â€” Formatting instructions help.
4. True â€” Ambiguity reduces consistency.
5. True â€” Core elements of a prompt.

---

## **Question 17: Context Specification**

Which describes the role of context in prompts?

**Options:**

1. True/False: Context improves relevance
2. True/False: Domain context prevents generic answers
3. True/False: Overly broad prompts weaken outputs
4. True/False: Context includes audience information
5. True/False: Context is optional in prompt engineering

**Explanation:**

1. True â€” More relevant responses.
2. True â€” Helps specialization.
3. True â€” Specific prompts yield better output.
4. True â€” Audience shapes tone.
5. False â€” Context is crucial.

---

## **Question 18: Example-Based Prompting**

Which describe few-shot prompting?

**Options:**

1. True/False: Providing examples improves pattern matching
2. True/False: Examples reduce hallucinations
3. True/False: Incorrect examples confuse the model
4. True/False: Examples help style transfer
5. True/False: Few-shot prompting is obsolete

**Explanation:**

1. True â€” Models mimic example format.
2. True â€” Grounding reduces errors.
3. True â€” Bad examples degrade output.
4. True â€” Used for format/style imitation.
5. False â€” Still widely used.

---

## **Question 19: Constraint Specification**

Which are valid constraints?

**Options:**

1. True/False: Word count
2. True/False: Tone/style
3. True/False: Structure/format
4. True/False: Audience specificity
5. True/False: Random constraints improve creativity

**Explanation:**

1. True â€” Common constraint.
2. True â€” Style control.
3. True â€” JSON/markdown/etc.
4. True â€” Targeting specific audiences.
5. False â€” Random constraints add noise.

---

## **Question 20: Iterative Refinement**

Which statements describe prompt iteration?

**Options:**

1. True/False: Prompts often need multiple passes
2. True/False: LLM outputs guide refinement
3. True/False: Refinement improves success rate
4. True/False: Perfect prompts work everywhere
5. True/False: Prompt iteration is standard in prototyping

**Explanation:**

1. True â€” Rarely correct on first attempt.
2. True â€” Output informs adjustments.
3. True â€” Improves reliability.
4. False â€” Prompts transfer poorly across tasks.
5. True â€” Iteration is essential.

---

## **Question 21: Role Prompting**

Which statements describe role prompting?

**Options:**

1. True/False: Assigns the model a persona
2. True/False: Helps enforce tone and expertise
3. True/False: Works equally across all models
4. True/False: May reduce hallucinations
5. True/False: Common in tutoring and simulation tasks

**Explanation:**

1. True â€” â€œYou are an expertâ€¦â€.
2. True â€” Influences answers.
3. False â€” Models vary in sensitivity.
4. True â€” Defined roles reduce drift.
5. True â€” Standard use case.

---

## **Question 22: Step-by-Step Structure**

Which describe step-by-step prompting?

**Options:**

1. True/False: Helps avoid skipped reasoning steps
2. True/False: Increases clarity
3. True/False: May improve correctness
4. True/False: Needed for all prompts
5. True/False: Works well in technical tasks

**Explanation:**

1. True â€” Reduces leaps in logic.
2. True â€” Structures reasoning.
3. True â€” Often increases accuracy.
4. False â€” Not required for simple tasks.
5. True â€” Coding/math benefit greatly.

---

## **Question 23: Chain-of-Thought**

Which are true about CoT prompting?

**Options:**

1. True/False: Reveals intermediate reasoning
2. True/False: Can improve reasoning accuracy
3. True/False: Always give CoT to end-users
4. True/False: CoT can hallucinate incorrect steps
5. True/False: CoT can be hidden internally

**Explanation:**

1. True â€” Produces visible reasoning.
2. True â€” Better logical structure.
3. False â€” End-users should not see model reasoning in safety contexts.
4. True â€” CoT can confabulate.
5. True â€” Models run internal CoT.

---

## **Question 24: Delimiters**

Why are delimiters useful?

**Options:**

1. True/False: Prevent prompt injection
2. True/False: Separate instructions from content
3. True/False: Improve safety
4. True/False: Improve reliability
5. True/False: Reduce hallucination risk

**Explanation:**

1. True â€” Clear boundaries resist injection.
2. True â€” Helps model parse tasks.
3. True â€” Safety improves by separation.
4. True â€” More consistent outputs.
5. True â€” Reduces confusion.

---

## **Question 25: Negative Instructions**

Which describe negative prompting?

**Options:**

1. True/False: Telling the model what NOT to do
2. True/False: Works in image models
3. True/False: Works less reliably in text models
4. True/False: Reduces unwanted behaviors
5. True/False: Guarantees avoidance of mistakes

**Explanation:**

1. True â€” Negative constraints.
2. True â€” Common in image generation.
3. True â€” Text models respond inconsistently.
4. True â€” Helps reduce issues.
5. False â€” No guarantees.

---

## **Question 26: Prompt Length**

Which describe prompt length considerations?

**Options:**

1. True/False: Longer prompts increase cost
2. True/False: Longer prompts always help
3. True/False: Short prompts may cause ambiguity
4. True/False: Balanced prompts perform best
5. True/False: Prompt length counts toward context window

**Explanation:**

1. True â€” More tokens = higher cost.
2. False â€” Unnecessary length can confuse.
3. True â€” Ask â†’ unclear.
4. True â€” Balanced design works best.
5. True â€” Uses up context.

---

## **Question 27: Prompt Injection**

Which statements describe injection?

**Options:**

1. True/False: A security risk
2. True/False: Occurs when user overrides instructions
3. True/False: Delimiters help prevent it
4. True/False: Models cannot detect injections reliably
5. True/False: Injection affects tool-using agents

**Explanation:**

1. True â€” Major LLM risk.
2. True â€” Overwrites system prompt.
3. True â€” Delimiters reduce risk.
4. True â€” No robust detection.
5. True â€” Agents especially vulnerable.

---

## **Question 28: Audience Specification**

Which statements relate to prompt audience?

**Options:**

1. True/False: Tone changes depending on audience
2. True/False: Technical depth depends on audience
3. True/False: Only writer-focused prompts need audience
4. True/False: Audience constraints reduce ambiguity
5. True/False: LLMs adjust explanations to audience

**Explanation:**

1. True â€” Tone shifts.
2. True â€” Expertise level varies.
3. False â€” All domains benefit.
4. True â€” Reduces confusion.
5. True â€” Models adapt phrasing.

---

## **Question 29: Style Transfer**

Which statements describe style prompts?

**Options:**

1. True/False: LLMs can mimic styles
2. True/False: Examples strengthen style matching
3. True/False: Models cannot combine styles
4. True/False: Style can be constrained
5. True/False: Style prompts help creative tasks

**Explanation:**

1. True â€” Strong capability.
2. True â€” Pattern matching improves.
3. False â€” They combine multiple styles well.
4. True â€” Constraints work.
5. True â€” Poetry, ads, scripts.

---

## **Question 30: Prompt Templates**

Which describe prompt templates?

**Options:**

1. True/False: Used for repeated tasks
2. True/False: Improve consistency
3. True/False: Reduce engineering time
4. True/False: Hurt flexibility
5. True/False: Standard in production systems

**Explanation:**

1. True â€” Reusable modules.
2. True â€” Standardization.
3. True â€” Less manual work.
4. False â€” Templates can accept parameters.
5. True â€” Best practice.

---

## **Question 31: Model Bias in Prompts**

Which describe prompt-driven bias?

**Options:**

1. True/False: Prompt phrasing can introduce bias
2. True/False: Examples can encode bias
3. True/False: Bias cannot be mitigated
4. True/False: Neutral phrasing reduces bias
5. True/False: LLMs can amplify biased prompts

**Explanation:**

1. True â€” Wording matters.
2. True â€” Examples influence patterns.
3. False â€” Mitigation exists.
4. True â€” Neutrality helps.
5. True â€” Pattern amplification.

---

## **Question 32: Meta-Prompts**

Which describe meta-prompts?

**Options:**

1. True/False: Prompts describing how to create prompts
2. True/False: Improve prompt quality
3. True/False: Are used to automate prompt generation
4. True/False: Are only needed for developers
5. True/False: Can chain into higher-level structures

**Explanation:**

1. True â€” Prompts about prompts.
2. True â€” Improve design.
3. True â€” Used in prompt-generation models.
4. False â€” Useful for advanced users too.
5. True â€” Meta-prompting creates frameworks.

---

## **Question 33: Safety Prompts**

Which statements describe safety prompts?

**Options:**

1. True/False: Used to enforce rules
2. True/False: Reduce harmful outputs
3. True/False: Are ignored by all models
4. True/False: Useful in content-moderation workflows
5. True/False: Guarantee full safety

**Explanation:**

1. True â€” Add guardrails.
2. True â€” Reduce risk.
3. False â€” Models obey them to varying degrees.
4. True â€” Frequently used in moderation pipelines.
5. False â€” Cannot guarantee full safety.

---

## **Question 34: Memory + History**

Which are true about conversational history?

**Options:**

1. True/False: Helps maintain continuity
2. True/False: Uses up context window
3. True/False: Models always remember earlier conversations
4. True/False: History can bias outputs
5. True/False: Resetting clears prior influence

**Explanation:**

1. True â€” Sustains conversation.
2. True â€” Counts as tokens.
3. False â€” Memory is not persistent without explicit storage.
4. True â€” Earlier text influences later tokens.
5. True â€” New session removes influence.

---

## **Question 35: Prompt Order Effects**

Which describe prompt ordering?

**Options:**

1. True/False: Order affects interpretation
2. True/False: Last instruction often dominates
3. True/False: System > user > assistant hierarchy applies
4. True/False: Reordered content can change meaning
5. True/False: Order has no effect

**Explanation:**

1. True â€” Order matters.
2. True â€” Recency effects exist.
3. True â€” Hierarchy enforced.
4. True â€” Semantic shifts occur.
5. False â€” Order is important.

---

# **SECTION 4 â€” RAG, VECTORS, EMBEDDINGS (10 Questions)**

---

## **Question 36: Embedding Basics**

Which describe embeddings?

**Options:**

1. True/False: Numeric vector representations of text
2. True/False: Capture semantic meaning
3. True/False: Used in similarity search
4. True/False: Require multimodal models
5. True/False: Input order affects embeddings

**Explanation:**

1. True â€” Core definition.
2. True â€” Represent meaning.
3. True â€” Used in search + clustering.
4. False â€” Text embeddings require text models only.
5. True â€” Word order influences meaning.

---

## **Question 37: Vector Databases**

Which describe vector DBs?

**Options:**

1. True/False: Store embeddings
2. True/False: Enable semantic search
3. True/False: Scale to millions of vectors
4. True/False: Are slower than SQL for semantic tasks
5. True/False: Used in RAG architectures

**Explanation:**

1. True â€” Store vectors.
2. True â€” Semantic retrieval.
3. True â€” Built for scale.
4. False â€” Faster for semantic similarity tasks.
5. True â€” Core component of RAG.

---

## **Question 38: RAG Architecture**

Which describe RAG?

**Options:**

1. True/False: Retrieves external documents
2. True/False: Improves factual grounding
3. True/False: Requires model retraining
4. True/False: Can use vector search
5. True/False: Reduces hallucinations

**Explanation:**

1. True â€” Retrieval step.
2. True â€” Uses external facts.
3. False â€” No retraining needed.
4. True â€” Standard implementation.
5. True â€” Better grounding.

---

## **Question 39: RAG Limitations**

Which are limitations?

**Options:**

1. True/False: Retrieval errors degrade output
2. True/False: Bad chunking harms performance
3. True/False: RAG eliminates hallucinations
4. True/False: LLM must interpret retrieved text
5. True/False: RAG needs quality documents

**Explanation:**

1. True â€” Garbage in â†’ garbage out.
2. True â€” Chunking affects recall/precision.
3. False â€” Reduces but does not eliminate.
4. True â€” LLM integrates retrieved data.
5. True â€” Document quality matters.

---

## **Question 40: Chunking Strategies**

Which describe chunking?

**Options:**

1. True/False: Splitting documents improves retrieval
2. True/False: Chunk size affects performance
3. True/False: Overlap reduces information loss
4. True/False: Large chunks always better
5. True/False: Optimal chunking varies by content

**Explanation:**

1. True â€” Enables targeted retrieval.
2. True â€” Influences recall/precision.
3. True â€” Overlapping helps context continuity.
4. False â€” Oversized chunks dilute relevance.
5. True â€” Depends on domain.

---

## **Question 41: Semantic Search vs Keyword Search**

Which differences are correct?

**Options:**

1. True/False: Semantic search uses embeddings
2. True/False: Keyword search uses string matching
3. True/False: Semantic search handles synonyms
4. True/False: Keyword search understands meaning
5. True/False: Semantic search is standard in RAG

**Explanation:**

1. True â€” Vector search.
2. True â€” Literal tokens.
3. True â€” Embeddings capture meaning.
4. False â€” Keyword search lacks semantic understanding.
5. True â€” Backbone of RAG.

---

## **Question 42: FAISS / HNSW / Annoy**

Which statements apply to vector search algorithms?

**Options:**

1. True/False: Designed for approximate nearest neighbor search
2. True/False: Scale efficiently to large datasets
3. True/False: Used to index embeddings
4. True/False: More efficient than brute force
5. True/False: Part of standard RAG systems

**Explanation:**

1. True â€” ANN algorithms.
2. True â€” Built for scalability.
3. True â€” Core indexing method.
4. True â€” Faster than O(n) comparisons.
5. True â€” Widely used.

---

## **Question 43: Embedding Drift**

Which statements describe embedding drift?

**Options:**

1. True/False: Occurs when embeddings from different models differ
2. True/False: Breaks vector search compatibility
3. True/False: Happens when model versions change
4. True/False: Fixed by re-embedding
5. True/False: Makes cross-model RAG difficult

**Explanation:**

1. True â€” Different models produce different vectors.
2. True â€” Breaks distance metrics.
3. True â€” Upgrades cause drift.
4. True â€” Must re-embed data.
5. True â€” Cross-model RAG is sensitive.

---

## **Question 44: Document Ranking**

Which describe retrieval ranking?

**Options:**

1. True/False: Score based on vector similarity
2. True/False: Poor ranking harms RAG answers
3. True/False: Ranking improves relevance
4. True/False: LLM must infer correct info from top-k
5. True/False: Retrieval is optional in RAG

**Explanation:**

1. True â€” Embedding similarity.
2. True â€” Bad ranking = irrelevant chunks.
3. True â€” Improves grounding.
4. True â€” LLM interprets retrieved results.
5. False â€” Retrieval is required for RAG.

---

# **SECTION 5 â€” LANGCHAIN, AGENTS & TOOL USE (5 Questions)**

---

## **Question 45: LangChain Basics**

Which statements describe LangChain accurately?

**Options:**

1. True/False: Provides chain-based workflows
2. True/False: Integrates vector databases
3. True/False: Integrates multiple LLMs
4. True/False: Cannot scale to production
5. True/False: Used to build agents

**Explanation:**

1. True â€” Chain architecture.
2. True â€” Vector DB integration.
3. True â€” Supports many model sources.
4. False â€” Scalable depending on infrastructure.
5. True â€” Core functionality.

---

## **Question 46: Agents**

Which describe LLM agents?

**Options:**

1. True/False: Use tools to complete tasks
2. True/False: Follow a plan-act-observe loop
3. True/False: Cannot hallucinate tools
4. True/False: Vulnerable to injection
5. True/False: Useful for automation

**Explanation:**

1. True â€” Tool invocation.
2. True â€” Standard agent loop.
3. False â€” Tool hallucination is common.
4. True â€” High vulnerability.
5. True â€” Workflow automation.

---

## **Question 47: Tool Use**

Which statements describe tool use?

**Options:**

1. True/False: LLM selects tools
2. True/False: Tools expand capabilities
3. True/False: Tools remove hallucination risks
4. True/False: Tools require structured inputs
5. True/False: LLMs reason about tool outputs

**Explanation:**

1. True â€” Agents choose tools.
2. True â€” External computation broadens capabilities.
3. False â€” Tools reduce but donâ€™t remove risk.
4. True â€” Tools often require strict formats.
5. True â€” LLMs interpret outputs.

---

## **Question 48: Evaluation of Agent Behavior**

Which describe evaluating agents?

**Options:**

1. True/False: Requires tracking intermediate steps
2. True/False: Must detect tool hallucination
3. True/False: Includes evaluating cost
4. True/False: Includes assessing latency
5. True/False: Agents never get stuck loops

**Explanation:**

1. True â€” Step analysis required.
2. True â€” Key evaluation concern.
3. True â€” Tools cost tokens.
4. True â€” Tool calls add latency.
5. False â€” Agents often loop.

---

## **Question 49: Chain vs Agent**

Which differences are correct?

**Options:**

1. True/False: Chains follow fixed steps
2. True/False: Agents decide steps dynamically
3. True/False: Chains are safer
4. True/False: Agents are more flexible
5. True/False: Chains cannot use tools

**Explanation:**

1. True â€” Fixed DAG.
2. True â€” Dynamic planning.
3. True â€” Lower hallucination risk.
4. True â€” Agents more capable.
5. False â€” Chains can call tools.

---

## **Question 50: LangChain Memory**

Which describe LangChain memory?

**Options:**

1. True/False: Stores conversation history
2. True/False: Uses tokens from the context window
3. True/False: Enables long-form interactions
4. True/False: Equivalent to model retraining
5. True/False: Can be combined with RAG

**Explanation:**

1. True â€” Stores aggregated history.
2. True â€” Counts toward tokens.
3. True â€” Facilitates continuity.
4. False â€” Memory â‰  training.
5. True â€” Memory + RAG works together.

---

# âœ… **DONE â€” 50 QUESTIONS, all accurate, up-to-date, exam-ready**

If you want, I can also generate:

âœ”ï¸ A **Canvas/Moodle import file**
âœ”ï¸ A **Google Forms upload CSV**
âœ”ï¸ A **printable PDF answer key**
âœ”ï¸ A **â€œstudent versionâ€ with answers removed**

Just say the word.

